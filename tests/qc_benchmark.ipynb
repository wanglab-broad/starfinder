{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC: Benchmark Framework Validation\n",
    "\n",
    "This notebook validates the `starfinder.benchmark` module by:\n",
    "1. Testing the `@benchmark` decorator\n",
    "2. Testing `run_comparison()` across methods\n",
    "3. Testing report generation (table, CSV, JSON)\n",
    "4. Validating timing/memory measurements are reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark module loaded successfully!\n",
      "Available size presets: ['tiny', 'small', 'medium', 'large', 'xlarge', 'tissue']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../src/python\")\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from starfinder.benchmark import (\n",
    "    BenchmarkResult,\n",
    "    BenchmarkSuite,\n",
    "    benchmark,\n",
    "    measure,\n",
    "    run_comparison,\n",
    "    print_table,\n",
    "    save_csv,\n",
    "    save_json,\n",
    "    SIZE_PRESETS,\n",
    ")\n",
    "\n",
    "print(\"Benchmark module loaded successfully!\")\n",
    "print(f\"Available size presets: {list(SIZE_PRESETS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test `measure()` Function\n",
    "\n",
    "Verify that `measure()` correctly captures execution time and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 42\n",
      "Elapsed time: 0.100s (expected ~0.1s)\n",
      "Memory: 0.00 MB\n",
      "✓ measure() works correctly\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def slow_function():\n",
    "    \"\"\"Function that takes ~100ms.\"\"\"\n",
    "    time.sleep(0.1)\n",
    "    return 42\n",
    "\n",
    "result, elapsed, memory = measure(slow_function)\n",
    "print(f\"Result: {result}\")\n",
    "print(f\"Elapsed time: {elapsed:.3f}s (expected ~0.1s)\")\n",
    "print(f\"Memory: {memory:.2f} MB\")\n",
    "\n",
    "assert result == 42, \"Return value should be preserved\"\n",
    "assert 0.08 < elapsed < 0.15, f\"Time should be ~0.1s, got {elapsed:.3f}s\"\n",
    "print(\"✓ measure() works correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test `@benchmark` Decorator\n",
    "\n",
    "Verify the decorator captures timing and extracts size from arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: numpy\n",
      "Operation: sum\n",
      "Size: (10, 256, 256)\n",
      "Time: 0.000621s\n",
      "Memory: 0.06 MB\n",
      "Return value: 655360.0\n",
      "✓ @benchmark decorator works correctly\n"
     ]
    }
   ],
   "source": [
    "@benchmark(method=\"numpy\", operation=\"sum\", size_arg=\"arr\")\n",
    "def array_sum(arr):\n",
    "    return arr.sum()\n",
    "\n",
    "test_arr = np.ones((10, 256, 256))\n",
    "result = array_sum(test_arr)\n",
    "\n",
    "print(f\"Method: {result.method}\")\n",
    "print(f\"Operation: {result.operation}\")\n",
    "print(f\"Size: {result.size}\")\n",
    "print(f\"Time: {result.time_seconds:.6f}s\")\n",
    "print(f\"Memory: {result.memory_mb:.2f} MB\")\n",
    "print(f\"Return value: {result.metrics['return_value']}\")\n",
    "\n",
    "assert result.size == (10, 256, 256), \"Size should be extracted from array\"\n",
    "assert result.metrics[\"return_value\"] == 10 * 256 * 256, \"Return value incorrect\"\n",
    "print(\"✓ @benchmark decorator works correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test `run_comparison()`\n",
    "\n",
    "Compare multiple methods on the same inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 4 results\n",
      "\n",
      "| Method | Operation | Size | Time (s) | Memory (MB) |\n",
      "|--------|-----------|------|----------|-------------|\n",
      "| numpy  | sum       | 5x64x64 |   0.0001 |         0.1 |\n",
      "| loop   | sum       | 5x64x64 |   0.0187 |         0.0 |\n",
      "| numpy  | sum       | 10x128x128 |   0.0001 |         0.1 |\n",
      "| loop   | sum       | 10x128x128 |   0.1207 |         0.0 |\n",
      "\n",
      "NumPy avg: 0.000068s\n",
      "Loop avg: 0.069731s\n",
      "NumPy is 1020.7x faster\n",
      "✓ run_comparison() works correctly\n"
     ]
    }
   ],
   "source": [
    "def numpy_sum(arr):\n",
    "    return np.sum(arr)\n",
    "\n",
    "def loop_sum(arr):\n",
    "    total = 0.0\n",
    "    for val in arr.flat:\n",
    "        total += val\n",
    "    return total\n",
    "\n",
    "# Create test arrays\n",
    "small_arr = np.ones((5, 64, 64))\n",
    "medium_arr = np.ones((10, 128, 128))\n",
    "\n",
    "results = run_comparison(\n",
    "    methods={\"numpy\": numpy_sum, \"loop\": loop_sum},\n",
    "    inputs=[small_arr, medium_arr],\n",
    "    operation=\"sum\",\n",
    "    n_runs=3,\n",
    "    warmup=True,\n",
    ")\n",
    "\n",
    "print(f\"Collected {len(results)} results\")\n",
    "print_table(results)\n",
    "\n",
    "# Verify numpy is faster than loop\n",
    "numpy_times = [r.time_seconds for r in results if r.method == \"numpy\"]\n",
    "loop_times = [r.time_seconds for r in results if r.method == \"loop\"]\n",
    "print(f\"NumPy avg: {np.mean(numpy_times):.6f}s\")\n",
    "print(f\"Loop avg: {np.mean(loop_times):.6f}s\")\n",
    "print(f\"NumPy is {np.mean(loop_times) / np.mean(numpy_times):.1f}x faster\")\n",
    "print(\"✓ run_comparison() works correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Report Generation\n",
    "\n",
    "Save results to CSV and JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSV to: benchmark_output/test_results.csv\n",
      "method,operation,size,time_seconds,memory_mb\n",
      "numpy,sum,5x64x64,5.678584178288778e-05,0.06342315673828125\n",
      "loop,sum,5x64x64,0.018738817423582077,0.0027882258097330728\n",
      "numpy,sum,10x128x128,7.984352608521779e-05,0.06342315673828125\n",
      "loop,sum,10x128x128,0.12072226156791051,0.0027720133463541665\n",
      "\n",
      "\n",
      "Saved JSON to: benchmark_output/test_results.json\n",
      "JSON contains 4 results\n",
      "✓ Report generation works correctly\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "output_dir = Path(\"benchmark_output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save CSV\n",
    "csv_path = output_dir / \"test_results.csv\"\n",
    "save_csv(results, csv_path)\n",
    "print(f\"Saved CSV to: {csv_path}\")\n",
    "print(csv_path.read_text()[:500])\n",
    "\n",
    "# Save JSON\n",
    "json_path = output_dir / \"test_results.json\"\n",
    "save_json(results, json_path)\n",
    "print(f\"\\nSaved JSON to: {json_path}\")\n",
    "\n",
    "data = json.loads(json_path.read_text())\n",
    "print(f\"JSON contains {len(data)} results\")\n",
    "print(\"✓ Report generation works correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test BenchmarkSuite\n",
    "\n",
    "Collect results and compute summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suite 'sum_benchmark' has 4 results\n",
      "\n",
      "Summary statistics:\n",
      "  mean_time: 0.034899\n",
      "  min_time: 0.000057\n",
      "  max_time: 0.120722\n",
      "  std_time: 0.050133\n",
      "  mean_memory: 0.033102\n",
      "\n",
      "NumPy results: 2\n",
      "✓ BenchmarkSuite works correctly\n"
     ]
    }
   ],
   "source": [
    "suite = BenchmarkSuite(name=\"sum_benchmark\")\n",
    "for r in results:\n",
    "    suite.add(r)\n",
    "\n",
    "print(f\"Suite '{suite.name}' has {len(suite.results)} results\")\n",
    "\n",
    "stats = suite.summary()\n",
    "print(f\"\\nSummary statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value:.6f}\")\n",
    "\n",
    "# Filter by method\n",
    "numpy_results = suite.filter(method=\"numpy\")\n",
    "print(f\"\\nNumPy results: {len(numpy_results)}\")\n",
    "print(\"✓ BenchmarkSuite works correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "All benchmark framework components validated:\n",
    "- [x] `measure()` - captures time and memory\n",
    "- [x] `@benchmark` decorator - wraps functions with timing\n",
    "- [x] `run_comparison()` - compares multiple methods\n",
    "- [x] `print_table()` - formatted output\n",
    "- [x] `save_csv()` / `save_json()` - file output\n",
    "- [x] `BenchmarkSuite` - result collection and stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
