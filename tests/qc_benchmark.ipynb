{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC: Benchmark Framework Validation\n",
    "\n",
    "This notebook validates the `starfinder.benchmark` module by:\n",
    "1. Testing the `@benchmark` decorator\n",
    "2. Testing `run_comparison()` across methods\n",
    "3. Testing report generation (table, CSV, JSON)\n",
    "4. Validating timing/memory measurements are reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../src/python\")\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from starfinder.benchmark import (\n",
    "    BenchmarkResult,\n",
    "    BenchmarkSuite,\n",
    "    benchmark,\n",
    "    measure,\n",
    "    run_comparison,\n",
    "    print_table,\n",
    "    save_csv,\n",
    "    save_json,\n",
    "    SIZE_PRESETS,\n",
    ")\n",
    "\n",
    "print(\"Benchmark module loaded successfully!\")\n",
    "print(f\"Available size presets: {list(SIZE_PRESETS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test `measure()` Function\n",
    "\n",
    "Verify that `measure()` correctly captures execution time and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def slow_function():\n",
    "    \"\"\"Function that takes ~100ms.\"\"\"\n",
    "    time.sleep(0.1)\n",
    "    return 42\n",
    "\n",
    "result, elapsed, memory = measure(slow_function)\n",
    "print(f\"Result: {result}\")\n",
    "print(f\"Elapsed time: {elapsed:.3f}s (expected ~0.1s)\")\n",
    "print(f\"Memory: {memory:.2f} MB\")\n",
    "\n",
    "assert result == 42, \"Return value should be preserved\"\n",
    "assert 0.08 < elapsed < 0.15, f\"Time should be ~0.1s, got {elapsed:.3f}s\"\n",
    "print(\"✓ measure() works correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test `@benchmark` Decorator\n",
    "\n",
    "Verify the decorator captures timing and extracts size from arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmark(method=\"numpy\", operation=\"sum\", size_arg=\"arr\")\n",
    "def array_sum(arr):\n",
    "    return arr.sum()\n",
    "\n",
    "test_arr = np.ones((10, 256, 256))\n",
    "result = array_sum(test_arr)\n",
    "\n",
    "print(f\"Method: {result.method}\")\n",
    "print(f\"Operation: {result.operation}\")\n",
    "print(f\"Size: {result.size}\")\n",
    "print(f\"Time: {result.time_seconds:.6f}s\")\n",
    "print(f\"Memory: {result.memory_mb:.2f} MB\")\n",
    "print(f\"Return value: {result.metrics['return_value']}\")\n",
    "\n",
    "assert result.size == (10, 256, 256), \"Size should be extracted from array\"\n",
    "assert result.metrics[\"return_value\"] == 10 * 256 * 256, \"Return value incorrect\"\n",
    "print(\"✓ @benchmark decorator works correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test `run_comparison()`\n",
    "\n",
    "Compare multiple methods on the same inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_sum(arr):\n",
    "    return np.sum(arr)\n",
    "\n",
    "def loop_sum(arr):\n",
    "    total = 0.0\n",
    "    for val in arr.flat:\n",
    "        total += val\n",
    "    return total\n",
    "\n",
    "# Create test arrays\n",
    "small_arr = np.ones((5, 64, 64))\n",
    "medium_arr = np.ones((10, 128, 128))\n",
    "\n",
    "results = run_comparison(\n",
    "    methods={\"numpy\": numpy_sum, \"loop\": loop_sum},\n",
    "    inputs=[small_arr, medium_arr],\n",
    "    operation=\"sum\",\n",
    "    n_runs=3,\n",
    "    warmup=True,\n",
    ")\n",
    "\n",
    "print(f\"Collected {len(results)} results\")\n",
    "print_table(results)\n",
    "\n",
    "# Verify numpy is faster than loop\n",
    "numpy_times = [r.time_seconds for r in results if r.method == \"numpy\"]\n",
    "loop_times = [r.time_seconds for r in results if r.method == \"loop\"]\n",
    "print(f\"NumPy avg: {np.mean(numpy_times):.6f}s\")\n",
    "print(f\"Loop avg: {np.mean(loop_times):.6f}s\")\n",
    "print(f\"NumPy is {np.mean(loop_times) / np.mean(numpy_times):.1f}x faster\")\n",
    "print(\"✓ run_comparison() works correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Report Generation\n",
    "\n",
    "Save results to CSV and JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output_dir = Path(\"benchmark_output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save CSV\n",
    "csv_path = output_dir / \"test_results.csv\"\n",
    "save_csv(results, csv_path)\n",
    "print(f\"Saved CSV to: {csv_path}\")\n",
    "print(csv_path.read_text()[:500])\n",
    "\n",
    "# Save JSON\n",
    "json_path = output_dir / \"test_results.json\"\n",
    "save_json(results, json_path)\n",
    "print(f\"\\nSaved JSON to: {json_path}\")\n",
    "\n",
    "data = json.loads(json_path.read_text())\n",
    "print(f\"JSON contains {len(data)} results\")\n",
    "print(\"✓ Report generation works correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test BenchmarkSuite\n",
    "\n",
    "Collect results and compute summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = BenchmarkSuite(name=\"sum_benchmark\")\n",
    "for r in results:\n",
    "    suite.add(r)\n",
    "\n",
    "print(f\"Suite '{suite.name}' has {len(suite.results)} results\")\n",
    "\n",
    "stats = suite.summary()\n",
    "print(f\"\\nSummary statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value:.6f}\")\n",
    "\n",
    "# Filter by method\n",
    "numpy_results = suite.filter(method=\"numpy\")\n",
    "print(f\"\\nNumPy results: {len(numpy_results)}\")\n",
    "print(\"✓ BenchmarkSuite works correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "All benchmark framework components validated:\n",
    "- [x] `measure()` - captures time and memory\n",
    "- [x] `@benchmark` decorator - wraps functions with timing\n",
    "- [x] `run_comparison()` - compares multiple methods\n",
    "- [x] `print_table()` - formatted output\n",
    "- [x] `save_csv()` / `save_json()` - file output\n",
    "- [x] `BenchmarkSuite` - result collection and stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
